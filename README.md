Electromyography and Gradient Boosting 
Gradient Boosting Machines are used very often by machine learning practioners to understand how GBMs work. Understanding the mathematical machinery can be tricky and these detials are needed in order to tune the hyper-parameters. The most common form of GBM that optimizes the mean squared error (MSE) is called the L2 loss or cost. A GBM is a composite model that combines the efforts of multiple weak models to create a strong model. Each additional weak model reduces the MSE of the overall model. Optimizing a model according to MSE makes it chase outliers because squaring the difference between targets and predicted values emphasizes extreme values. If an outlier cant be removed, you optimize the mean absolute erorr (MAE) which is also called the L1 loss or cost. 
![image](https://user-images.githubusercontent.com/117037344/204190635-1e036059-277a-418b-8cee-602a66c7c0d6.png)
